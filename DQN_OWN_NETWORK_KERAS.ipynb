{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define setup\n",
    "run = 'V015'\n",
    "learner = 'valid_moves' # valid_moves to include double check; self_learner to try let agent learn himself which moves is valid\n",
    "model_type = 'cnn'\n",
    "train = True\n",
    "evaluation = True\n",
    "starting = True\n",
    "callbacks = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'c:\\users\\richa\\.conda\\envs\\connectx_37\\lib\\site-packages')\n",
    "\n",
    "# disable infos and warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from kaggle_environments import evaluate, make, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, MaxPooling2D, Activation, Flatten, Input, Conv2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup connect x environment\n",
    "env = make(\"connectx\", debug=True)\n",
    "print(list(env.agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.run([\"negamax\", \"random\"])\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_agent(observation, config):\n",
    "    # define grid to play\n",
    "    grid = np.asarray(observation.board).reshape(config.rows, config.columns)\n",
    "    grid = np.expand_dims(grid, 0)\n",
    "    grid = np.expand_dims(grid, 0)\n",
    "\n",
    "    move = model.predict(grid)\n",
    "    move = int(np.argmax(move))\n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_agent_val(observation, config):\n",
    "    valid_moves = [col for col in range(config.columns) if observation.board[col] == 0]\n",
    "    \n",
    "    # define grid to play\n",
    "    grid = np.asarray(observation.board).reshape(config.rows, config.columns)\n",
    "    grid = np.expand_dims(grid, 0)\n",
    "    grid = np.expand_dims(grid, 0)\n",
    "\n",
    "    \n",
    "    move = model.predict(grid)\n",
    "    move = int(np.argmax(move))\n",
    "    \n",
    "    if move in valid_moves:\n",
    "        move = move\n",
    "    else:\n",
    "        move = random.choice(valid_moves)\n",
    "        \n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour(gym.Env):\n",
    "    def __init__(self, learner, starting):\n",
    "        # initialize environment with oppenent agent to play against\n",
    "        if learner == 'self_learner':\n",
    "            agent2 = my_agent\n",
    "        elif learner == 'valid_moves':\n",
    "            agent2 = my_agent_val\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        if starting:\n",
    "            self.env = ks_env.train([None, agent2])\n",
    "            self.mark = 1\n",
    "            self.opp_mark = 2\n",
    "        else:\n",
    "            self.env = ks_env.train([agent2, None])\n",
    "            self.mark = 2\n",
    "            self.opp_mark = 1\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        \n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = gym.spaces.Discrete(self.columns)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=2, \n",
    "                                            shape=(1, self.rows, self.columns), dtype=np.int)\n",
    "        \n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.min_reward = -30 # penalization\n",
    "        self.max_reward = 10\n",
    "        self.reward_range = (self.min_reward, self.max_reward)\n",
    "        \n",
    "    \"\"\"grid functions\n",
    "    functions to check if multiple own marks were achieved or multiple opp_marks were stopped\n",
    "    \"\"\"\n",
    "    def check_left(self, grid, row, action, mark, opp_mark):\n",
    "        \"\"\"get reward - in case action was set on very left, pass and leave reward as it is\"\"\"\n",
    "        reward = 1/42\n",
    "        # grid = np.asarray(self.obs[\"board\"]).reshape(self.rows, self.columns)\n",
    "        if grid[row, action-1] == mark and action-1 >= 0:\n",
    "            reward = 1/3\n",
    "        else:\n",
    "            pass\n",
    "        if grid[row, action-1] == mark and grid[row, action-2] == mark and action-2 >= 0:\n",
    "            reward = 1\n",
    "        elif grid[row, action-1] == opp_mark and grid[row, action-2] == opp_mark and action-2 >= 0:\n",
    "            reward = 1\n",
    "        else:\n",
    "            pass\n",
    "        return reward\n",
    "    \n",
    "    def check_right(self, grid, row, action, mark, opp_mark):\n",
    "        \"\"\"get reward - in case of index error, mark was set on very right which can not result in multiple marks to right \n",
    "        but was also not a mistake (simply a move on the righthandside) --> set reward to 1/42\"\"\"\n",
    "        reward = 1/42\n",
    "        try:\n",
    "            if grid[row, action+1] == mark:\n",
    "                reward = 1/3\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            if grid[row, action+1] == mark and grid[row, action+2] == mark:\n",
    "                reward = 1\n",
    "            elif grid[row, action+1] == opp_mark and grid[row, action+2] == opp_mark:\n",
    "                reward = 1\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return reward\n",
    "    \n",
    "    def check_bottom(self, grid, row, action, mark, opp_mark):\n",
    "        \"\"\"get reward - in case of index error, mark was set on bottom which can not result in multiple marks to bottom\n",
    "        but was also not a mistake (simply the beginning of the game) --> set reward to 1/42\"\"\"\n",
    "        reward = 1/42\n",
    "        try:\n",
    "            if grid[row+1, action] == mark:\n",
    "                reward = 1/3\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            if grid[row+1, action] == mark and grid[row+2, action] == mark:\n",
    "                reward = 1\n",
    "            elif grid[row+1, action] == opp_mark and grid[row+2, action] == opp_mark:\n",
    "                reward = 1\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return reward\n",
    "    \n",
    "    def check_bottom_left(self, grid, row, action, mark, opp_mark):\n",
    "        \"\"\"get reward - in case of index error, mark was set on bottom which can not result in que to bottom but was also not\n",
    "        a mistake (simply the beginning of the game) --> set reward to 1/42\"\"\"\n",
    "        reward = 1/42\n",
    "        try:\n",
    "            if grid[row+1, action-1] == mark and action-2 >= 0:\n",
    "                reward = 1/3\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            if grid[row+1, action-1] == mark and grid[row+2, action-2] == mark and action-2 >= 0:\n",
    "                reward = 1\n",
    "            elif grid[row+1, action-1] == opp_mark and grid[row+2, action-2] == opp_mark and action-2 >= 0:\n",
    "                reward = 1\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return reward\n",
    "        \n",
    "    def check_bottom_right(self, grid, row, action, mark, opp_mark):\n",
    "        \"\"\"get reward - in case of index error, mark was set on bottom which can not result in que to bottom but was also not\n",
    "        a mistake (simply the beginning of the game) --> set reward to 1/42\"\"\"\n",
    "        reward = 1/42\n",
    "        try:\n",
    "            if grid[row+1, action+1] == mark:\n",
    "                reward = 1/3\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            if grid[row+1, action+1] == mark and grid[row+2, action+2] == mark:\n",
    "                reward = 1\n",
    "            elif grid[row+1, action+1] == opp_mark and grid[row+2, action+2] == opp_mark:\n",
    "                reward = 1\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return reward\n",
    "    \n",
    "    def check_top_left(self, grid, row, action, mark, opp_mark):\n",
    "        \"\"\"get reward - in case mark was set to very top and very left, pass and leave reward as it is\"\"\"\n",
    "        reward = 1/42\n",
    "        if grid[row-1, action-1] == mark and row-1 >= 0 and action-1 >= 0:\n",
    "            reward = 1/3\n",
    "        else:\n",
    "            pass\n",
    "        if grid[row-1, action-1] == mark and grid[row-2, action-2] == mark and row-2 >= 0 and action-2 >= 0:\n",
    "            reward = 1\n",
    "        elif grid[row-1, action-1] == opp_mark and grid[row-2, action-2] == opp_mark and row-2 >= 0 and action-2 >= 0:\n",
    "            reward = 1\n",
    "        else:\n",
    "            pass\n",
    "        return reward\n",
    "        \n",
    "    def check_top_right(self, grid, row, action, mark, opp_mark):\n",
    "        \"\"\"get reward - in case mark was set to very top and very right, pass and leave reward as it is\"\"\"\n",
    "        reward = 1/42\n",
    "        try:\n",
    "            if grid[row-1, action+1] == mark and row-1 >= 0:\n",
    "                reward = 1/3\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            if grid[row-1, action+1] == mark and grid[row-2, action+2] == mark and row-2 >= 0:\n",
    "                reward = 1\n",
    "            elif grid[row-1, action+1] == opp_mark and grid[row-2, action+2] == opp_mark and row-2 >= 0:\n",
    "                reward = 1\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return reward\n",
    "\n",
    "    def check_grid_reward(self, action, mark, opp_mark):\n",
    "        # initialize state of game\n",
    "        grid = np.asarray(self.obs['board']).reshape(self.rows, self.columns)\n",
    "        action_col = grid[:,action] # to define marks and later action_row\n",
    "        marks = np.squeeze(np.asarray(np.where(action_col==mark))) # own marks in action col\n",
    "        if marks.size > 1: # get row of top action mark --> action row\n",
    "            action_row = marks[0]\n",
    "        elif marks.size == 1:\n",
    "            action_row = marks\n",
    "        else: # start of game, does not apply with env rules\n",
    "            action_row = False\n",
    "        \n",
    "        grid_reward = 0\n",
    "        grid_reward = grid_reward + self.check_left(grid, action_row, action, mark, opp_mark)\n",
    "        grid_reward = grid_reward + self.check_right(grid, action_row, action, mark, opp_mark)\n",
    "        grid_reward = grid_reward + self.check_bottom(grid, action_row, action, mark, opp_mark)\n",
    "        grid_reward = grid_reward + self.check_bottom_left(grid, action_row, action, mark, opp_mark)\n",
    "        grid_reward = grid_reward + self.check_bottom_right(grid, action_row, action, mark, opp_mark)\n",
    "        grid_reward = grid_reward + self.check_top_right(grid, action_row, action, mark, opp_mark)\n",
    "        grid_reward = grid_reward + self.check_top_left(grid, action_row, action, mark, opp_mark)\n",
    "\n",
    "        return grid_reward\n",
    "    \n",
    "    \"\"\"setting functions\"\"\"    \n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns)\n",
    "    \n",
    "    def change_reward(self, old_reward, done, action):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return self.max_reward\n",
    "        elif done: # The opponent won the game\n",
    "            return self.max_reward * (-1)\n",
    "        else: # get grid reward (self made function --> reward is dependent on sourroundings)\n",
    "            grid_reward = self.check_grid_reward(action, self.mark, self.opp_mark)\n",
    "            return grid_reward\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done, action)\n",
    "        else: # End the game and penalize agent --> not valid move\n",
    "            reward, done, _ = self.min_reward, True, {}\n",
    "#         print(f'printing reward: {reward}')\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns), reward, done, _\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_AGENT:\n",
    "    def __init__(self, env, model_type,\n",
    "                 target_model_update, lr, num_steps, policy, memory_limit, window_length, steps_warmup):\n",
    "        self.env = env\n",
    "        self.states = self.env.observation_space.shape\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.target_model_update = target_model_update\n",
    "        self.lr = lr\n",
    "        self.optimizer = Adam(self.lr)\n",
    "        self.num_steps = num_steps\n",
    "        self.memory_limit = memory_limit\n",
    "        self.window_length = window_length\n",
    "        self.steps_warmup = steps_warmup\n",
    "        self.checkpoint_path = 'log/checkpoints/' + model_type + '_model_' + version + '.hdf5'\n",
    "        self.weights_path = 'log/weights/' + model_type + '_model_' + version + '.hdf5'\n",
    "        self.plot_path = 'log/plots/' + model_type + '_model_' + version\n",
    "\n",
    "\n",
    "    def build_conv_model(self):\n",
    "        input_layer = Input(shape=(self.states))\n",
    "        x = Conv2D(filters=16, kernel_size=5, strides=(3, 3), padding='same',\n",
    "                   data_format='channels_first', activation='tanh')(input_layer)\n",
    "        x = BatchNormalization(axis=1)(x)\n",
    "        x = Conv2D(filters=8, kernel_size=5, strides=(3, 3), padding='same',\n",
    "                   data_format='channels_first', activation='tanh')(x)\n",
    "        x = BatchNormalization(axis=1)(x)\n",
    "        x = Conv2D(filters=8, kernel_size=5, strides=(3, 3), padding='same',\n",
    "                   data_format='channels_first', activation='tanh')(x)\n",
    "        x = BatchNormalization(axis=1)(x)\n",
    "        x = Conv2D(filters=16, kernel_size=5, strides=(3, 3), padding='same',\n",
    "                   data_format='channels_first', activation='tanh')(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        # x = Dense(16, activation='relu')(x)\n",
    "        output_layer = Dense(self.num_actions)(x)\n",
    "\n",
    "        self.model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        # print(self.model.summary())\n",
    "        return self.model\n",
    "\n",
    "    def build_dense_model(self):\n",
    "        input_layer = Input(shape=(self.states))\n",
    "        x = Flatten()(input_layer)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(16, activation='relu')(x)\n",
    "        output_layer = Dense(self.num_actions, activation='linear')(x)\n",
    "        self.model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        print(self.model.summary())\n",
    "        return self.model\n",
    "    \n",
    "    def build_callbacks(self):\n",
    "        checkpoint = ModelCheckpoint(filepath=self.checkpoint_path, save_weights_only=True, monitor='episode_reward', mode='max',\n",
    "                 save_best_only=True, verbose=1)\n",
    "        stopper = EarlyStopping(monitor='episode_reward', min_delta=0, patience=5, verbose=1, mode='max',\n",
    "                                baseline=None, restore_best_weights=False)\n",
    "        # self.callbacks = [checkpoint, stopper]\n",
    "        self.callbacks = [checkpoint]\n",
    "        return self.callbacks\n",
    "    \n",
    "    def build_agent(self):\n",
    "        self.policy = BoltzmannQPolicy()\n",
    "        self.memory = SequentialMemory(limit=self.memory_limit, window_length=self.window_length)\n",
    "        self.dqn = DQNAgent(model=self.model, memory=self.memory, policy=self.policy,\n",
    "                            nb_actions=self.num_actions, nb_steps_warmup=self.steps_warmup,\n",
    "                            target_model_update=self.target_model_update)\n",
    "        return self.dqn\n",
    "    \n",
    "    def train_agent(self, verbose):\n",
    "        self.dqn.compile(self.optimizer, metrics=['mae'])\n",
    "        if callbacks:\n",
    "            self.history = self.dqn.fit(self.env, nb_steps=self.num_steps, callbacks=self.callbacks, visualize=False,\n",
    "                                    verbose=verbose)\n",
    "        else:\n",
    "            self.history = self.dqn.fit(self.env, nb_steps=self.num_steps, visualize=False, verbose=verbose)\n",
    "        self.dqn.save_weights(self.weights_path, overwrite=True)\n",
    "        \n",
    "        # plot rewards of each game\n",
    "        plt.plot(self.history.epoch, self.history.history['episode_reward'])\n",
    "        plt.title('Reward over the Games played')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.show()\n",
    "        plt.savefig(self.plot_path + '_games_reward.png')\n",
    "        \n",
    "        # plot mean reward over the last 100 games\n",
    "        print(f'target model update: {target_model_update}\\\n",
    "                learing rate: {lr}\\\n",
    "                memory limit: {memory_limit}')\n",
    "        mean_rewards = []\n",
    "        for i in range(0, len(self.history.history['episode_reward']), 100):\n",
    "            mean_rewards.append(np.mean(self.history.history['episode_reward'][i:i+99]))\n",
    "        plt.plot(list(range(len(mean_rewards))), mean_rewards)\n",
    "        plt.title('Average Reward over the last 100 Games played')\n",
    "        plt.xlabel('Number of 100 Games')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.show()\n",
    "        plt.savefig(self.plot_path + '_avg_reward.png')\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.weights_path)\n",
    "        return self.model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This agent random chooses a non-empty column.\n",
    "def random_agent(observation, config):\n",
    "    valid_moves = [col for col in range(config.columns) if observation.board[col] == 0]\n",
    "    move = random.choice(valid_moves)\n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    \n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    \n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    \n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Train Agent\"\"\"\n",
    "if train:\n",
    "    env = ConnectFour(learner, starting)\n",
    "\n",
    "    # define grid\n",
    "    target_model_updates = [0.01, 0.001]\n",
    "    lrs = [0.01, 0.001]\n",
    "    memory_limits = [1_000, 10_000]\n",
    "    num_opp_updates = [5, 10]\n",
    "    \n",
    "    for i, (target_model_update, lr, memory_limit, num_opp_update) in enumerate(\n",
    "        zip(target_model_updates, lrs, memory_limits, num_opp_updates)):\n",
    "        version = run + '_' + str(i)\n",
    "\n",
    "        # define model, define agent, train agent and adapt weights\n",
    "        DQN = DQN_AGENT(env, model_type,\n",
    "                        target_model_update=target_model_update, lr=lr, num_steps=99_800, policy=BoltzmannQPolicy,\n",
    "                        memory_limit=memory_limit, window_length=1, steps_warmup=100)\n",
    "        \n",
    "        # update model to train against\n",
    "        for update_opp in range(num_opp_update):\n",
    "            print(f'opponent has now been updatet: {update_opp} times')\n",
    "            # initialize weights randomly for first run\n",
    "            if update_opp == 0:\n",
    "                if model_type == 'cnn':\n",
    "                    model = DQN.build_conv_model()\n",
    "                elif model_type == 'dense':\n",
    "                    model = DQN.build_dense_model()\n",
    "                    \n",
    "                    \n",
    "            # load next state after training for at least one full run\n",
    "            elif update_opp > 0:\n",
    "                if model_type == 'cnn':\n",
    "                    model = DQN.build_conv_model()\n",
    "                    DQN.load_weights()\n",
    "                elif model_type == 'dense':\n",
    "                    model = DQN.build_dense_model()\n",
    "                    DQN.load_weights()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if callbacks:\n",
    "                callbacks = DQN.build_callbacks()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # build agent and start training\n",
    "            dqn = DQN.build_agent()\n",
    "            history = DQN.train_agent(verbose=0)            \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate Agent\"\"\"\n",
    "if evaluation:\n",
    "    env = ConnectFour(learner, starting)\n",
    "    \n",
    "    # set values from best run of grid search\n",
    "    target_model_update = 0.001\n",
    "    lr = 0.001\n",
    "    memory_limit = 10_000\n",
    "    version = run + '_' + '1'\n",
    "\n",
    "    DQN = DQN_AGENT(env, model_type,\n",
    "                    target_model_update=target_model_update, lr=lr, num_steps=1, policy=BoltzmannQPolicy,\n",
    "                    memory_limit=memory_limit, window_length=1, steps_warmup=1)\n",
    "    \n",
    "    if model_type == 'cnn':\n",
    "        model = DQN.build_conv_model()\n",
    "    elif model_type == 'dense':\n",
    "        model = DQN.build_dense_model()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    dqn = DQN.build_agent()\n",
    "    model = DQN.load_weights()\n",
    "\n",
    "    # Create the game environment\n",
    "    env = make(\"connectx\", debug=True)\n",
    "    \n",
    "    # initialize agent\n",
    "    if learner == 'self_learner':\n",
    "        agent = my_agent\n",
    "    elif learner == 'valid_moves':\n",
    "        agent = my_agent_val\n",
    "\n",
    "    # play against random agent\n",
    "    env.run([agent, 'random'])\n",
    "\n",
    "    # Show the game\n",
    "    env.render(mode=\"ipython\")\n",
    "\n",
    "    # evaluate performance over n games\n",
    "    if starting:\n",
    "        print('win percentages against random agent when starting')\n",
    "        get_win_percentages(agent, 'random', n_rounds=100)\n",
    "        print('\\nwin percentages against negamax when starting')\n",
    "        get_win_percentages(agent, 'negamax', n_rounds=50)\n",
    "    else:\n",
    "        print('win percentages against random agent when going second')\n",
    "        get_win_percentages('random', agent, n_rounds=100)\n",
    "        print('\\nwin percentages against negamax when going second')\n",
    "        get_win_percentages('negamax', agent, n_rounds=50)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play against my agent\n",
    "env.play([None, my_agent], width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectx_kerasrl",
   "language": "python",
   "name": "connectx_kerasrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
